{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced Melting Point Prediction (Ensemble + GPU)\n",
                "\n",
                "## Strategy\n",
                "1.  **SMILES-only**: Focus on molecular structure.\n",
                "2.  **Enhanced Features**: RDKit Descriptors + Morgan Fingerprints (Radius 2, 1024 bits).\n",
                "3.  **Ensemble Model**: Combine **CatBoost** and **XGBoost** predictions.\n",
                "4.  **GPU Acceleration**: Enabled for both models.\n",
                "\n",
                "## Prerequisite\n",
                "**Run the cell below to install necessary libraries:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# INSTALL LIBRARIES\n",
                "!pip install rdkit pingouin optuna catboost lightgbm xgboost openpyxl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import pingouin as pg\n",
                "import optuna\n",
                "from catboost import CatBoostRegressor\n",
                "from xgboost import XGBRegressor\n",
                "from rdkit import Chem\n",
                "from rdkit.Chem import Descriptors, AllChem, GraphDescriptors, rdFingerprintGenerator\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "import os\n",
                "\n",
                "# configuration\n",
                "pd.set_option(\"display.max_columns\", 50)\n",
                "pd.set_option(\"display.width\", 120)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data (Auto-Download)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check environment and load data\n",
                "if os.path.exists('/kaggle/input/melting-point/train.csv'):\n",
                "    # Running on Kaggle Kernel\n",
                "    print(\"Detected Kaggle Kernel environment.\")\n",
                "    data_path = '/kaggle/input/melting-point/'\n",
                "elif os.path.exists('train.csv'):\n",
                "    # Local file exists\n",
                "    print(\"Detected local dataset.\")\n",
                "    data_path = './'\n",
                "else:\n",
                "    # Try downloading via Kaggle API (Requires kaggle.json)\n",
                "    print(\"Dataset not found. Attempting download via Kaggle API...\")\n",
                "    print(\"NOTE: You need to upload your 'kaggle.json' API token if on Colab.\")\n",
                "    \n",
                "    try:\n",
                "        !pip install -q kaggle\n",
                "        !kaggle competitions download -c melting-point\n",
                "        !unzip -o melting-point.zip\n",
                "        data_path = './'\n",
                "        print(\"Download successful.\")\n",
                "    except Exception as e:\n",
                "        print(\"\\nERROR: Could not download data automatically.\")\n",
                "        print(\"Please upload 'train.csv', 'test.csv', and 'sample_submission.csv' manually.\")\n",
                "        data_path = './'\n",
                "\n",
                "# Load Data\n",
                "try:\n",
                "    df_train = pd.read_csv(f\"{data_path}train.csv\", sep=\",\")[['SMILES', 'Tm']]\n",
                "    test_df = pd.read_csv(f\"{data_path}test.csv\")\n",
                "    submission_df = pd.read_csv(f\"{data_path}sample_submission.csv\")\n",
                "    print(f\"\\nLoaded Train shape: {df_train.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"\\nCRITICAL ERROR: Data files still not found. Please check your setup.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering (Descriptors + Fingerprints)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_mol_features(smiles):\n",
                "    mol = Chem.MolFromSmiles(smiles)\n",
                "    if not mol:\n",
                "        return None\n",
                "    \n",
                "    # 1. Extensive Descriptors\n",
                "    features = {\n",
                "        'MolWt': Descriptors.MolWt(mol),\n",
                "        'LogP': Descriptors.MolLogP(mol),\n",
                "        'NumHDonors': Descriptors.NumHDonors(mol),\n",
                "        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
                "        'TPSA': Descriptors.TPSA(mol),\n",
                "        'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
                "        'RingCount': Descriptors.RingCount(mol),\n",
                "        'HeavyAtomCount': Descriptors.HeavyAtomCount(mol),\n",
                "        'NumValenceElectrons': Descriptors.NumValenceElectrons(mol),\n",
                "        'BertzCT': GraphDescriptors.BertzCT(mol),\n",
                "        'HallKierAlpha': GraphDescriptors.HallKierAlpha(mol),\n",
                "    }\n",
                "    \n",
                "    # 2. Morgan Fingerprints (Radius 2, 1024 bits) - USING NEW GENERATOR API\n",
                "    mfgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n",
                "    fp = mfgen.GetFingerprint(mol)\n",
                "    fp_bits = list(fp)\n",
                "    \n",
                "    for i, bit in enumerate(fp_bits):\n",
                "        features[f'fp_{i}'] = bit\n",
                "        \n",
                "    return features\n",
                "\n",
                "print(\"Extracting features for Train set (this may take a minute)...\")\n",
                "train_features_df = df_train['SMILES'].apply(get_mol_features).apply(pd.Series)\n",
                "train_full = pd.concat([df_train, train_features_df], axis=1)\n",
                "\n",
                "print(\"Extracting features for Test set...\")\n",
                "test_features_df = test_df['SMILES'].apply(get_mol_features).apply(pd.Series)\n",
                "test_full = pd.concat([test_df, test_features_df], axis=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Training (Ensemble: CatBoost + XGBoost) with GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data\n",
                "features = [c for c in train_full.columns if c not in ['id', 'SMILES', 'Tm']]\n",
                "X = train_full[features].fillna(0)\n",
                "y = train_full['Tm']\n",
                "X_test = test_full[features].fillna(0)\n",
                "\n",
                "# K-Fold Cross Validation\n",
                "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "oof_preds = np.zeros(len(X))\n",
                "test_preds_cb = np.zeros(len(X_test))\n",
                "test_preds_xgb = np.zeros(len(X_test))\n",
                "mae_scores = []\n",
                "\n",
                "print(\"Starting Cross-Validation on GPU...\")\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
                "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
                "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
                "    \n",
                "    # --- CatBoost (GPU) ---\n",
                "    cb = CatBoostRegressor(\n",
                "        iterations=5000,\n",
                "        learning_rate=0.03,\n",
                "        depth=7,\n",
                "        l2_leaf_reg=5,\n",
                "        loss_function='MAE',\n",
                "        verbose=0,\n",
                "        random_seed=42,\n",
                "        task_type=\"GPU\", # ENABLE GPU\n",
                "        devices='0'      # Use first GPU\n",
                "    )\n",
                "    cb.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=200)\n",
                "    cb_val_pred = cb.predict(X_val)\n",
                "    test_preds_cb += cb.predict(X_test) / kf.get_n_splits()\n",
                "    \n",
                "    # --- XGBoost (GPU) ---\n",
                "    xgb = XGBRegressor(\n",
                "        n_estimators=5000,\n",
                "        learning_rate=0.03,\n",
                "        max_depth=7,\n",
                "        reg_alpha=1,\n",
                "        reg_lambda=5,\n",
                "        n_jobs=-1,\n",
                "        random_state=42,\n",
                "        early_stopping_rounds=200,\n",
                "        tree_method='hist',     # CHANGED: 'gpu_hist' -> 'hist' (Newer XGBoost)\n",
                "        device='cuda'           # Explicitly set device\n",
                "    )\n",
                "    xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
                "    xgb_val_pred = xgb.predict(X_val)\n",
                "    test_preds_xgb += xgb.predict(X_test) / kf.get_n_splits()\n",
                "    \n",
                "    # --- Ensemble (Average) ---\n",
                "    ensemble_val_pred = (cb_val_pred * 0.5) + (xgb_val_pred * 0.5)\n",
                "    oof_preds[val_idx] = ensemble_val_pred\n",
                "    \n",
                "    mae = mean_absolute_error(y_val, ensemble_val_pred)\n",
                "    mae_scores.append(mae)\n",
                "    print(f\"Fold {fold+1} MAE: {mae:.4f}\")\n",
                "\n",
                "print(f\"Average MAE: {np.mean(mae_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "final_test_preds = (test_preds_cb * 0.5) + (test_preds_xgb * 0.5)\n",
                "submission_df['Tm'] = final_test_preds\n",
                "submission_df.to_csv('submission_ensemble_gpu.csv', index=False)\n",
                "print(\"Saved submission_ensemble_gpu.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
