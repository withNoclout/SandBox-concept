{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Preference Prediction - Baseline Notebook\n",
                "\n",
                "This notebook implements a baseline solution for the LLM Classification challenge.\n",
                "It uses a **DeBERTa-v3** model in a Cross-Encoder configuration to predict which response is preferred."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "# We only install what is missing. We avoid upgrading torch/numpy/pandas to prevent conflicts.\n",
                "# We pin pyarrow and rich because 'datasets' might try to upgrade them, breaking other kaggle packages.\n",
                "!pip install -q \"pyarrow<20\" \"rich<14\" transformers datasets sentencepiece"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, AdamW, get_linear_schedule_with_warmup\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    MODEL_NAME = \"microsoft/deberta-v3-xsmall\" # Change to 'base' or 'large' for better results\n",
                "    MAX_LENGTH = 512\n",
                "    BATCH_SIZE = 4\n",
                "    EPOCHS = 1\n",
                "    LEARNING_RATE = 2e-5\n",
                "    SEED = 42\n",
                "    \n",
                "def set_seed(seed):\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    np.random.seed(seed)\n",
                "\n",
                "set_seed(Config.SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LLMPreferenceDataset(Dataset):\n",
                "    def __init__(self, df, tokenizer, max_length=1024, is_train=True):\n",
                "        self.df = df\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.is_train = is_train\n",
                "        self.texts = []\n",
                "        self.labels = []\n",
                "        \n",
                "        for _, row in df.iterrows():\n",
                "            prompt = str(row['prompt'])\n",
                "            res_a = str(row['response_a'])\n",
                "            res_b = str(row['response_b'])\n",
                "            \n",
                "            # Format: [CLS] Prompt [SEP] Response A [SEP] Response B [SEP]\n",
                "            text = f\"{prompt} {tokenizer.sep_token} {res_a} {tokenizer.sep_token} {res_b}\"\n",
                "            self.texts.append(text)\n",
                "            \n",
                "            if self.is_train:\n",
                "                if row['winner_model_a'] == 1:\n",
                "                    label = 0\n",
                "                elif row['winner_model_b'] == 1:\n",
                "                    label = 1\n",
                "                else:\n",
                "                    label = 2\n",
                "                self.labels.append(label)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.texts)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        text = self.texts[idx]\n",
                "        encoding = self.tokenizer(\n",
                "            text,\n",
                "            truncation=True,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        item = {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten()\n",
                "        }\n",
                "        \n",
                "        if self.is_train:\n",
                "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "            \n",
                "        return item"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LLMPreferenceModel(nn.Module):\n",
                "    def __init__(self, model_name, num_labels=3):\n",
                "        super(LLMPreferenceModel, self).__init__()\n",
                "        config = AutoConfig.from_pretrained(model_name)\n",
                "        config.num_labels = num_labels\n",
                "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
                "\n",
                "    def forward(self, input_ids, attention_mask, labels=None):\n",
                "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train():\n",
                "    # Load Data\n",
                "    # Assuming files are in the current directory\n",
                "    if os.path.exists(\"train.csv\"):\n",
                "        df = pd.read_csv(\"train.csv\")\n",
                "        # df = df.sample(n=2000, random_state=42) # Uncomment for quick testing\n",
                "    else:\n",
                "        print(\"train.csv not found. Please upload dataset.\")\n",
                "        return\n",
                "\n",
                "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
                "    full_dataset = LLMPreferenceDataset(df, tokenizer, max_length=Config.MAX_LENGTH)\n",
                "    \n",
                "    train_size = int(0.9 * len(full_dataset))\n",
                "    val_size = len(full_dataset) - train_size\n",
                "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE)\n",
                "    \n",
                "    model = LLMPreferenceModel(Config.MODEL_NAME)\n",
                "    model.to(device)\n",
                "    \n",
                "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
                "    total_steps = len(train_loader) * Config.EPOCHS\n",
                "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
                "    \n",
                "    best_val_loss = float('inf')\n",
                "    \n",
                "    for epoch in range(Config.EPOCHS):\n",
                "        print(f\"\\nEpoch {epoch + 1}/{Config.EPOCHS}\")\n",
                "        model.train()\n",
                "        total_train_loss = 0\n",
                "        \n",
                "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            \n",
                "            model.zero_grad()\n",
                "            outputs = model(input_ids, attention_mask, labels=labels)\n",
                "            loss = outputs.loss\n",
                "            loss.backward()\n",
                "            \n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            optimizer.step()\n",
                "            scheduler.step()\n",
                "            total_train_loss += loss.item()\n",
                "            \n",
                "        avg_train_loss = total_train_loss / len(train_loader)\n",
                "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        total_val_loss = 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for batch in val_loader:\n",
                "                input_ids = batch['input_ids'].to(device)\n",
                "                attention_mask = batch['attention_mask'].to(device)\n",
                "                labels = batch['labels'].to(device)\n",
                "                outputs = model(input_ids, attention_mask, labels=labels)\n",
                "                total_val_loss += outputs.loss.item()\n",
                "                \n",
                "        avg_val_loss = total_val_loss / len(val_loader)\n",
                "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
                "        \n",
                "        if avg_val_loss < best_val_loss:\n",
                "            best_val_loss = avg_val_loss\n",
                "            torch.save(model.state_dict(), \"best_model.pt\")\n",
                "            print(\"Saved Best Model\")\n",
                "            \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def inference(model=None):\n",
                "    if not os.path.exists(\"test.csv\"):\n",
                "        print(\"test.csv not found.\")\n",
                "        return\n",
                "        \n",
                "    df = pd.read_csv(\"test.csv\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
                "    dataset = LLMPreferenceDataset(df, tokenizer, max_length=Config.MAX_LENGTH, is_train=False)\n",
                "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
                "    \n",
                "    if model is None:\n",
                "        model = LLMPreferenceModel(Config.MODEL_NAME)\n",
                "        if os.path.exists(\"best_model.pt\"):\n",
                "            model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
                "        model.to(device)\n",
                "        \n",
                "    model.eval()\n",
                "    all_probs = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            outputs = model(input_ids, attention_mask)\n",
                "            probs = torch.softmax(outputs.logits, dim=1)\n",
                "            all_probs.append(probs.cpu().numpy())\n",
                "            \n",
                "    all_probs = np.concatenate(all_probs, axis=0)\n",
                "    \n",
                "    submission = pd.DataFrame({\n",
                "        'id': df['id'],\n",
                "        'winner_model_a': all_probs[:, 0],\n",
                "        'winner_model_b': all_probs[:, 1],\n",
                "        'winner_tie': all_probs[:, 2]\n",
                "    })\n",
                "    \n",
                "    submission.to_csv(\"submission.csv\", index=False)\n",
                "    print(\"Submission saved!\")\n",
                "    print(submission.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Pipeline\n",
                "if __name__ == \"__main__\":\n",
                "    trained_model = train()\n",
                "    inference(trained_model)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}